---
title: "2025-06-12 notes"
description: Notes
---

Optimisers ready to use:

### [Optax](https://optax.readthedocs.io/en/latest/api/optimizers.html)

- Adam
- LBFGS

### [JAXopt](https://jaxopt.github.io/stable/unconstrained.html)

- Gradient descent
- BFGS
- Newton-CG
- NonlinearCG
- Trust region (trust-exact, trust-ncg, and trust-krylov)

### [Somax](https://jaxopt.github.io/stable/unconstrained.html)

- Gauss--Newton (exact, stoachastic and incremental)
- Natural Gradient
- "Stochastic Quasi-Newton Framework (SQN)"
- [AdaHessian](https://doi.org/10.1609/aaai.v35i12.17275) ("An Adaptive Second Order Optimizer for Machine Learning")
- [Sophia](https://arxiv.org/abs/2305.14342) ("A Scalable Stochastic Second-order Optimizer for Language Model Pre-training")


### Custom

- Pure Newton